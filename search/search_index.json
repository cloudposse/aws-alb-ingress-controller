{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS ALB Ingress Controller \u00b6 NOTE: This controller is in beta state as we attempt to move to our first 1.0 release. The current image version is 1.0-beta.7 . Please file any issues you find and note the version used. The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . This project was originated by Ticketmaster and CoreOS as part of Ticketmaster's move to AWS and CoreOS Tectonic. Learn more about Ticketmaster's Kubernetes initiative from Justin Dean's video at Tectonic Summit . This project was donated to Kubernetes SIG-AWS to allow AWS, CoreOS, Ticketmaster and other SIG-AWS contributors to officially maintain the project. SIG-AWS reached this consensus on June 1, 2018. Getting started \u00b6 To get started with the controller, see our walkthrough for echoserver . Setup \u00b6 See controller setup on how to install ALB ingress controller See external-dns setup for how to setup the external-dns to manage route 53 records. Building \u00b6 For details on building this project, see BUILDING.md . License \u00b6","title":"Welcome"},{"location":"#aws-alb-ingress-controller","text":"NOTE: This controller is in beta state as we attempt to move to our first 1.0 release. The current image version is 1.0-beta.7 . Please file any issues you find and note the version used. The AWS ALB Ingress Controller satisfies Kubernetes ingress resources by provisioning Application Load Balancers . This project was originated by Ticketmaster and CoreOS as part of Ticketmaster's move to AWS and CoreOS Tectonic. Learn more about Ticketmaster's Kubernetes initiative from Justin Dean's video at Tectonic Summit . This project was donated to Kubernetes SIG-AWS to allow AWS, CoreOS, Ticketmaster and other SIG-AWS contributors to officially maintain the project. SIG-AWS reached this consensus on June 1, 2018.","title":"AWS ALB Ingress Controller"},{"location":"#getting-started","text":"To get started with the controller, see our walkthrough for echoserver .","title":"Getting started"},{"location":"#setup","text":"See controller setup on how to install ALB ingress controller See external-dns setup for how to setup the external-dns to manage route 53 records.","title":"Setup"},{"location":"#building","text":"For details on building this project, see BUILDING.md .","title":"Building"},{"location":"#license","text":"","title":"License"},{"location":"BUILDING/","text":"Building \u00b6 Download this repo locally \u00b6 $ go get -d github.com/kubernetes-sigs/aws-alb-ingress-controller $ cd $GOPATH /src/github.com/kubernetes-sigs/aws-alb-ingress-controller Build the binary and container with the Makefile \u00b6 $ make clean ; make Verify the local container is known to your Docker daemon \u00b6 $ docker images | grep -i alb-ingress-controller quay.io/coreos/alb-ingress-controller 1 .0-beta.4 78f356144e33 20 minutes ago 47 .4MB Version can vary based on what's in the Makefile. If you wish to push to your own repo for testing, you can change the version and repo details in the Makefile then do a docker push . Running locally \u00b6 If you'd like to make modifications and run this controller locally for the purpose of debugging, the following script can be used a basis for how to bootstrap the controller. It assumes you have a default kubeconfig for your cluster at ~/.kube/config . #!/bin/bash KUBECTL_PROXY_PID = $( pgrep -fx \"kubectl proxy\" ) echo $KUBECTL_PROXY_PID if [[ -z $KUBECTL_PROXY_PID ]] then echo \"kubectl proxy was not running. Starting it.\" else echo \"Found kubectl proxy is running. Killing it. Starting it.\" kill $KUBECTL_PROXY_PID fi kubectl proxy & >/dev/null & kubectl apply -f ./examples/echoservice/echoserver-namespace.yaml kubectl apply -f ./examples/echoservice/echoserver-deployment.yaml kubectl apply -f ./examples/echoservice/echoserver-service.yaml kubectl apply -f ./examples/echoservice/echoserver-ingress2.yaml $ make server Or on MacOS $ OS = darwin make server $ AWS_REGION = us-west-2 POD_NAME = alb-ingress-controller POD_NAMESPACE = kube-system go run cmd/main.go --apiserver-host = http://localhost:8001 --cluster-name = devcluster","title":"Building"},{"location":"BUILDING/#building","text":"","title":"Building"},{"location":"BUILDING/#download-this-repo-locally","text":"$ go get -d github.com/kubernetes-sigs/aws-alb-ingress-controller $ cd $GOPATH /src/github.com/kubernetes-sigs/aws-alb-ingress-controller","title":"Download this repo locally"},{"location":"BUILDING/#build-the-binary-and-container-with-the-makefile","text":"$ make clean ; make","title":"Build the binary and container with the Makefile"},{"location":"BUILDING/#verify-the-local-container-is-known-to-your-docker-daemon","text":"$ docker images | grep -i alb-ingress-controller quay.io/coreos/alb-ingress-controller 1 .0-beta.4 78f356144e33 20 minutes ago 47 .4MB Version can vary based on what's in the Makefile. If you wish to push to your own repo for testing, you can change the version and repo details in the Makefile then do a docker push .","title":"Verify the local container is known to your Docker daemon"},{"location":"BUILDING/#running-locally","text":"If you'd like to make modifications and run this controller locally for the purpose of debugging, the following script can be used a basis for how to bootstrap the controller. It assumes you have a default kubeconfig for your cluster at ~/.kube/config . #!/bin/bash KUBECTL_PROXY_PID = $( pgrep -fx \"kubectl proxy\" ) echo $KUBECTL_PROXY_PID if [[ -z $KUBECTL_PROXY_PID ]] then echo \"kubectl proxy was not running. Starting it.\" else echo \"Found kubectl proxy is running. Killing it. Starting it.\" kill $KUBECTL_PROXY_PID fi kubectl proxy & >/dev/null & kubectl apply -f ./examples/echoservice/echoserver-namespace.yaml kubectl apply -f ./examples/echoservice/echoserver-deployment.yaml kubectl apply -f ./examples/echoservice/echoserver-service.yaml kubectl apply -f ./examples/echoservice/echoserver-ingress2.yaml $ make server Or on MacOS $ OS = darwin make server $ AWS_REGION = us-west-2 POD_NAME = alb-ingress-controller POD_NAMESPACE = kube-system go run cmd/main.go --apiserver-host = http://localhost:8001 --cluster-name = devcluster","title":"Running locally"},{"location":"CODE_OF_CONDUCT/","text":"Kubernetes Community Code of Conduct Kubernetes follows the CNCF Code of Conduct","title":"CODE OF CONDUCT"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 Thanks for taking the time to join our community and start contributing! The Contributor Guide provides detailed instructions on how to get your ideas and bug fixes seen and accepted. Please remember to sign the CNCF CLA and read and observe the Code of Conduct .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"Thanks for taking the time to join our community and start contributing! The Contributor Guide provides detailed instructions on how to get your ideas and bug fixes seen and accepted. Please remember to sign the CNCF CLA and read and observe the Code of Conduct .","title":"Contributing"},{"location":"ROADMAP/","text":"v1.1.0 \u00b6 support sharing ALB between ingresses across namespace support AWS Cognito","title":"Roadmap"},{"location":"ROADMAP/#v110","text":"support sharing ALB between ingresses across namespace support AWS Cognito","title":"v1.1.0"},{"location":"api/configuration/","text":"Configuration \u00b6 This document covers configuration of the ALB Ingress Controller. AWS API Access \u00b6 To perform operations, the controller must have required IAM role capabilities for accessing and provisioning ALB resources. There are many ways to achieve this, such as loading AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY as environment variables or using kube2iam . A sample IAM policy, with the minimum permissions to run the controller, can be found in examples/alb-iam-policy.json . Setting Ingress Resource Scope \u00b6 By default, all ingress resources in your cluster are seen by the controller. However, only ingress resources that contain the required annotations will be satisfied by the ALB Ingress Controller. You can further limit the ingresses your controller has access to. The options available are limiting the ingress class ( ingress.class ) or limiting the namespace watched ( --watch-namespace= ). Each approach is detailed below. Limiting Ingress Class \u00b6 Setting the kubernetes.io/ingress.class annotation allows for classification of ingress resources and is especially helpful when running multiple ingress controllers in the same cluster. See Using Multiple Ingress Controllers for more details. An example of the container spec portion of the controller, only listening for resources with the class \"alb\", would be as follows. spec : containers : - args : - /server - --ingress-class=alb Now, only ingress resources with the appropriate annotation are picked up, as seen below. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : echoserver namespace : echoserver annotations : alb.ingress.kubernetes.io/port : \"8080,9000\" kubernetes.io/ingress.class : \"alb\" spec : ... Limiting Namespaces \u00b6 Setting the --watch-namespace argument constrains the controller's scope to a single namespace. Ingress events outside of the namespace specified are not be seen by the controller. An example of the container spec, for a controller watching only the default namespace, is as follows. spec : containers : - args : - /server - --watch-namespace=default Currently, you can set only 1 namespace to watch in this flag. See this Kubernetes issue for more details. Limiting External Namespaces \u00b6 Setting the --restrict-scheme boolean flag to true will enable the ALB controller to check the configmap named alb-ingress-controller-internet-facing-ingresses for a list of approved ingresses before provisioning ALBs with an internet-facing scheme. Here is an example of that ConfigMap: apiVersion : v1 data : mynamespace : my-ingress-name, my-ingress-name-2 myothernamespace : my-other-ingress-name kind : ConfigMap metadata : name : alb-ingress-controller-internet-facing-ingresses That ConfigMap is kept in default if unspecified, but can moved to another with the ALB_CONTROLLER_RESTRICT_SCHEME_CONFIG_NAMESPACE environment variable. This can also be passed to the command line via the restrict-scheme-namespace flag. Tags \u00b6 Setting the --default-tags argument adds arbitrary tags to ALBs and target groups managed by the ingress controller. spec : containers : - args : - /server - --default-tags=mykey=myvalue,otherkey=othervalue","title":"Configuration"},{"location":"api/configuration/#configuration","text":"This document covers configuration of the ALB Ingress Controller.","title":"Configuration"},{"location":"api/configuration/#aws-api-access","text":"To perform operations, the controller must have required IAM role capabilities for accessing and provisioning ALB resources. There are many ways to achieve this, such as loading AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY as environment variables or using kube2iam . A sample IAM policy, with the minimum permissions to run the controller, can be found in examples/alb-iam-policy.json .","title":"AWS API Access"},{"location":"api/configuration/#setting-ingress-resource-scope","text":"By default, all ingress resources in your cluster are seen by the controller. However, only ingress resources that contain the required annotations will be satisfied by the ALB Ingress Controller. You can further limit the ingresses your controller has access to. The options available are limiting the ingress class ( ingress.class ) or limiting the namespace watched ( --watch-namespace= ). Each approach is detailed below.","title":"Setting Ingress Resource Scope"},{"location":"api/configuration/#limiting-ingress-class","text":"Setting the kubernetes.io/ingress.class annotation allows for classification of ingress resources and is especially helpful when running multiple ingress controllers in the same cluster. See Using Multiple Ingress Controllers for more details. An example of the container spec portion of the controller, only listening for resources with the class \"alb\", would be as follows. spec : containers : - args : - /server - --ingress-class=alb Now, only ingress resources with the appropriate annotation are picked up, as seen below. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : echoserver namespace : echoserver annotations : alb.ingress.kubernetes.io/port : \"8080,9000\" kubernetes.io/ingress.class : \"alb\" spec : ...","title":"Limiting Ingress Class"},{"location":"api/configuration/#limiting-namespaces","text":"Setting the --watch-namespace argument constrains the controller's scope to a single namespace. Ingress events outside of the namespace specified are not be seen by the controller. An example of the container spec, for a controller watching only the default namespace, is as follows. spec : containers : - args : - /server - --watch-namespace=default Currently, you can set only 1 namespace to watch in this flag. See this Kubernetes issue for more details.","title":"Limiting Namespaces"},{"location":"api/configuration/#limiting-external-namespaces","text":"Setting the --restrict-scheme boolean flag to true will enable the ALB controller to check the configmap named alb-ingress-controller-internet-facing-ingresses for a list of approved ingresses before provisioning ALBs with an internet-facing scheme. Here is an example of that ConfigMap: apiVersion : v1 data : mynamespace : my-ingress-name, my-ingress-name-2 myothernamespace : my-other-ingress-name kind : ConfigMap metadata : name : alb-ingress-controller-internet-facing-ingresses That ConfigMap is kept in default if unspecified, but can moved to another with the ALB_CONTROLLER_RESTRICT_SCHEME_CONFIG_NAMESPACE environment variable. This can also be passed to the command line via the restrict-scheme-namespace flag.","title":"Limiting External Namespaces"},{"location":"api/configuration/#tags","text":"Setting the --default-tags argument adds arbitrary tags to ALBs and target groups managed by the ingress controller. spec : containers : - args : - /server - --default-tags=mykey=myvalue,otherkey=othervalue","title":"Tags"},{"location":"api/ingress/","text":"Ingress Resources \u00b6 This document covers how ingress resources work in relation to The ALB Ingress Controller. Ingress Behavior \u00b6 Periodically, ingress update events are seen by the controller. The controller retains a list of all ingress resources it knows about, along with the current state of AWS components that satisfy them. When an update event is fired, the controller re-scans the list of ingress resources known to the cluster and determines, by comparing the list to its previously stored one, the ingresses requiring deletion, creation or modification. An example ingress, from example/2048/2048-ingress.yaml is as follows. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : \"nginx-ingress\" namespace : \"2048-game\" annotations : kubernetes.io/ingress.class : alb labels : app : 2048-nginx-ingress spec : rules : - host : 2048.example.com http : paths : - path : / backend : serviceName : \"service-2048\" servicePort : 80 The host field specifies the eventual Route 53-managed domain that will route to this service. The service, service-2048, must be of type NodePort (see ../examples/echoservice/echoserver-service.yaml ) in order for the provisioned ALB to route to it. If no NodePort exists, the controller will not attempt to provision resources in AWS. For details on purpose of annotations seen above, see Annotations . Annotations \u00b6 The ALB Ingress Controller is configured by Annotations on the Ingress and Service resource objects. alb.ingress.kubernetes.io/load-balancer-attributes alb.ingress.kubernetes.io/backend-protocol alb.ingress.kubernetes.io/certificate-arn alb.ingress.kubernetes.io/healthcheck-interval-seconds alb.ingress.kubernetes.io/healthcheck-path alb.ingress.kubernetes.io/healthcheck-port alb.ingress.kubernetes.io/healthcheck-protocol alb.ingress.kubernetes.io/healthcheck-timeout-seconds alb.ingress.kubernetes.io/healthy-threshold-count alb.ingress.kubernetes.io/unhealthy-threshold-count alb.ingress.kubernetes.io/listen-ports alb.ingress.kubernetes.io/inbound-cidrs alb.ingress.kubernetes.io/target-type alb.ingress.kubernetes.io/scheme alb.ingress.kubernetes.io/security-groups alb.ingress.kubernetes.io/subnets alb.ingress.kubernetes.io/success-codes alb.ingress.kubernetes.io/tags alb.ingress.kubernetes.io/target-group-attributes alb.ingress.kubernetes.io/ip-address-type alb.ingress.kubernetes.io/ssl-policy alb.ingress.kubernetes.io/actions.<ACTION NAME> load-balancer-attributes : Defines Load Balancer Attributes that should be applied to the ALB. This can be used to enable the S3 access logs feature of the ALB. Example: alb.ingress.kubernetes.io/load-balancer-attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=my-access-log-bucket backend-protocol : Enables selection of protocol for ALB to use to connect to backend service. When omitted, HTTP is used. certificate-arn : Enables HTTPS and uses the certificate defined, based on arn, stored in your AWS Certificate Manager . healthcheck-interval-seconds : The approximate amount of time, in seconds, between health checks of an individual target. The default is 15 seconds. healthcheck-path : The ping path that is the destination on the targets for health checks. The default is /. healthcheck-port : The port the load balancer uses when performing health checks on targets. The default is traffic-port, which indicates the port on which each target receives traffic from the load balancer. healthcheck-protocol : The protocol the load balancer uses when performing health checks on targets. The default is the HTTP protocol. healthcheck-timeout-seconds : The amount of time, in seconds, during which no response from a target means a failed health check. The default is 5 seconds. healthcheck-healthy-threshold-count : The number of consecutive health checks successes required before considering an unhealthy target healthy. The default is 2. healthcheck-unhealthy-threshold-count : The number of consecutive health check failures required before considering a target unhealthy. The default is 2. listen-ports : Defines the ports the ALB will expose. It defaults to [{\"HTTP\": 80}] unless a certificate ARN is defined, then it is [{\"HTTPS\": 443}] . Uses a format as follows '[{\"HTTP\":8080,\"HTTPS\": 443}]'. inbound-cidrs : Defines the CIDR whitelist for ingress traffic to ALB. It defaults to 0.0.0.0/0 .(Will be ignored if security-groups is specified) target-type : Defines if the EC2 instance ID or the pod IP are used in the managed Target Groups. Defaults to instance . Valid options are instance and ip . With instance the Target Group targets are <ec2 instance id>:<node port> , for ip the targets are <pod ip>:<pod port> . ip is to be used when the pod network is routable and can be reached by the ALB. scheme : Defines whether an ALB should be internal or internet-facing . See Load balancer scheme in the AWS documentation for more details. security-groups : Security groups that should be applied to the ALB instance. These can be referenced by security group IDs or the name tag associated with each security group. Example ID values are sg-723a380a,sg-a6181ede,sg-a5181edd . Example tag values are appSG, webSG . When the annotation is not present, the controller will create a security group with appropriate ports allowing access to 0.0.0.0/0 and attached to the ALB. It will also create a security group for instances that allows all TCP traffic when the source is the security group created for the ALB. subnets : The subnets where the ALB instance should be deployed. Must include 2 subnets, each in a different availability zone . These can be referenced by subnet IDs or the name tag associated with the subnet. Example values for subnet IDs are subnet-a4f0098e,subnet-457ed533,subnet-95c904cd . Example values for name tags are: webSubnet,appSubnet . If subnets are not specified the ALB controller will attempt to detect qualified subnets. This qualification is done by locating subnets that match the following criteria. kubernetes.io/cluster/$CLUSTER_NAME where $CLUSTER_NAME is the same cluster name specified on the ingress controller. The value of this tag must be shared or owned . kubernetes.io/role/internal-elb should be set to 1 or an empty tag value for internal load balancers. kubernetes.io/role/elb should be set to 1 or an empty tag value for internet-facing load balancers. After subnets matching the above 2 tags have been located, they are checked to ensure 2 or more are in unique AZs, otherwise the ALB will not be created. If 2 subnets share the same AZ, only 1 of the 2 is used. success-codes : Defines the HTTP status code that should be expected when doing health checks against the defined healthcheck-path . When omitted, 200 is used. tags : Defines AWS Tags that should be applied to ALB instance, Target groups and SecurityGroups. target-group-attributes : Defines Target Group Attributes which can be assigned to the Target Groups. Currently these are applied equally to all target groups in the ingress. ip-address-type : The IP address type thats used to either route IPv4 traffic only or to route both IPv4 and IPv6 traffic. Can be either dualstack or ipv4 . When omitted ipv4 is used. ssl-policy : Defines the Security Policy that should be assigned to the ALB, allowing you to control the protocol and ciphers. alb.ingress.kubernetes.io/actions.\\ : Provides a method for configuring custom actions on a listener, such as for Redirect Actions . The <ACTION NAME> in the annotation must match the serviceName in the ingress rules. The value of the annotation is the JSON spec of the action. See the Action type for documentation on what should be in the JSON. NOTE you must set the servicePort to use-annotation . For a fixed-response, use alb.ingress.kubernetes.io/actions.fixed-response-error: '{\"Type\": \"fixed-response\", \"FixedResponseConfig\": {\"ContentType\":\"text/plain\", \"StatusCode\":\"503\", \"MessageBody\":\"503 error text\"}}' with a serviceName : fixed - response - error and servicePort : use - annotation . For a HTTP to HTTPS redirect, use alb.ingress.kubernetes.io/actions.redirect: {\"Type\": \"redirect\", \"RedirectConfig\": { \"Protocol\": \"HTTPS\", \"StatusCode\": \"HTTP_301\"}} with serviceName : redirect and servicePort : use - annotation . Services \u00b6 A subset of these annotations are supported on Services. This is used to customize the Target Group created for the Service. If a Service has no annotations, the Target Group options will default to the same options configured on the Ingress. Optional Service Annotations \u00b6 alb.ingress.kubernetes.io/backend-protocol alb.ingress.kubernetes.io/healthcheck-interval-seconds alb.ingress.kubernetes.io/healthcheck-path alb.ingress.kubernetes.io/healthcheck-port alb.ingress.kubernetes.io/healthcheck-protocol alb.ingress.kubernetes.io/healthcheck-timeout-seconds alb.ingress.kubernetes.io/healthy-threshold-count alb.ingress.kubernetes.io/unhealthy-threshold-count alb.ingress.kubernetes.io/target-type alb.ingress.kubernetes.io/success-codes alb.ingress.kubernetes.io/target-group-attributes","title":"Ingress Resources"},{"location":"api/ingress/#ingress-resources","text":"This document covers how ingress resources work in relation to The ALB Ingress Controller.","title":"Ingress Resources"},{"location":"api/ingress/#ingress-behavior","text":"Periodically, ingress update events are seen by the controller. The controller retains a list of all ingress resources it knows about, along with the current state of AWS components that satisfy them. When an update event is fired, the controller re-scans the list of ingress resources known to the cluster and determines, by comparing the list to its previously stored one, the ingresses requiring deletion, creation or modification. An example ingress, from example/2048/2048-ingress.yaml is as follows. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : \"nginx-ingress\" namespace : \"2048-game\" annotations : kubernetes.io/ingress.class : alb labels : app : 2048-nginx-ingress spec : rules : - host : 2048.example.com http : paths : - path : / backend : serviceName : \"service-2048\" servicePort : 80 The host field specifies the eventual Route 53-managed domain that will route to this service. The service, service-2048, must be of type NodePort (see ../examples/echoservice/echoserver-service.yaml ) in order for the provisioned ALB to route to it. If no NodePort exists, the controller will not attempt to provision resources in AWS. For details on purpose of annotations seen above, see Annotations .","title":"Ingress Behavior"},{"location":"api/ingress/#annotations","text":"The ALB Ingress Controller is configured by Annotations on the Ingress and Service resource objects. alb.ingress.kubernetes.io/load-balancer-attributes alb.ingress.kubernetes.io/backend-protocol alb.ingress.kubernetes.io/certificate-arn alb.ingress.kubernetes.io/healthcheck-interval-seconds alb.ingress.kubernetes.io/healthcheck-path alb.ingress.kubernetes.io/healthcheck-port alb.ingress.kubernetes.io/healthcheck-protocol alb.ingress.kubernetes.io/healthcheck-timeout-seconds alb.ingress.kubernetes.io/healthy-threshold-count alb.ingress.kubernetes.io/unhealthy-threshold-count alb.ingress.kubernetes.io/listen-ports alb.ingress.kubernetes.io/inbound-cidrs alb.ingress.kubernetes.io/target-type alb.ingress.kubernetes.io/scheme alb.ingress.kubernetes.io/security-groups alb.ingress.kubernetes.io/subnets alb.ingress.kubernetes.io/success-codes alb.ingress.kubernetes.io/tags alb.ingress.kubernetes.io/target-group-attributes alb.ingress.kubernetes.io/ip-address-type alb.ingress.kubernetes.io/ssl-policy alb.ingress.kubernetes.io/actions.<ACTION NAME> load-balancer-attributes : Defines Load Balancer Attributes that should be applied to the ALB. This can be used to enable the S3 access logs feature of the ALB. Example: alb.ingress.kubernetes.io/load-balancer-attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=my-access-log-bucket backend-protocol : Enables selection of protocol for ALB to use to connect to backend service. When omitted, HTTP is used. certificate-arn : Enables HTTPS and uses the certificate defined, based on arn, stored in your AWS Certificate Manager . healthcheck-interval-seconds : The approximate amount of time, in seconds, between health checks of an individual target. The default is 15 seconds. healthcheck-path : The ping path that is the destination on the targets for health checks. The default is /. healthcheck-port : The port the load balancer uses when performing health checks on targets. The default is traffic-port, which indicates the port on which each target receives traffic from the load balancer. healthcheck-protocol : The protocol the load balancer uses when performing health checks on targets. The default is the HTTP protocol. healthcheck-timeout-seconds : The amount of time, in seconds, during which no response from a target means a failed health check. The default is 5 seconds. healthcheck-healthy-threshold-count : The number of consecutive health checks successes required before considering an unhealthy target healthy. The default is 2. healthcheck-unhealthy-threshold-count : The number of consecutive health check failures required before considering a target unhealthy. The default is 2. listen-ports : Defines the ports the ALB will expose. It defaults to [{\"HTTP\": 80}] unless a certificate ARN is defined, then it is [{\"HTTPS\": 443}] . Uses a format as follows '[{\"HTTP\":8080,\"HTTPS\": 443}]'. inbound-cidrs : Defines the CIDR whitelist for ingress traffic to ALB. It defaults to 0.0.0.0/0 .(Will be ignored if security-groups is specified) target-type : Defines if the EC2 instance ID or the pod IP are used in the managed Target Groups. Defaults to instance . Valid options are instance and ip . With instance the Target Group targets are <ec2 instance id>:<node port> , for ip the targets are <pod ip>:<pod port> . ip is to be used when the pod network is routable and can be reached by the ALB. scheme : Defines whether an ALB should be internal or internet-facing . See Load balancer scheme in the AWS documentation for more details. security-groups : Security groups that should be applied to the ALB instance. These can be referenced by security group IDs or the name tag associated with each security group. Example ID values are sg-723a380a,sg-a6181ede,sg-a5181edd . Example tag values are appSG, webSG . When the annotation is not present, the controller will create a security group with appropriate ports allowing access to 0.0.0.0/0 and attached to the ALB. It will also create a security group for instances that allows all TCP traffic when the source is the security group created for the ALB. subnets : The subnets where the ALB instance should be deployed. Must include 2 subnets, each in a different availability zone . These can be referenced by subnet IDs or the name tag associated with the subnet. Example values for subnet IDs are subnet-a4f0098e,subnet-457ed533,subnet-95c904cd . Example values for name tags are: webSubnet,appSubnet . If subnets are not specified the ALB controller will attempt to detect qualified subnets. This qualification is done by locating subnets that match the following criteria. kubernetes.io/cluster/$CLUSTER_NAME where $CLUSTER_NAME is the same cluster name specified on the ingress controller. The value of this tag must be shared or owned . kubernetes.io/role/internal-elb should be set to 1 or an empty tag value for internal load balancers. kubernetes.io/role/elb should be set to 1 or an empty tag value for internet-facing load balancers. After subnets matching the above 2 tags have been located, they are checked to ensure 2 or more are in unique AZs, otherwise the ALB will not be created. If 2 subnets share the same AZ, only 1 of the 2 is used. success-codes : Defines the HTTP status code that should be expected when doing health checks against the defined healthcheck-path . When omitted, 200 is used. tags : Defines AWS Tags that should be applied to ALB instance, Target groups and SecurityGroups. target-group-attributes : Defines Target Group Attributes which can be assigned to the Target Groups. Currently these are applied equally to all target groups in the ingress. ip-address-type : The IP address type thats used to either route IPv4 traffic only or to route both IPv4 and IPv6 traffic. Can be either dualstack or ipv4 . When omitted ipv4 is used. ssl-policy : Defines the Security Policy that should be assigned to the ALB, allowing you to control the protocol and ciphers. alb.ingress.kubernetes.io/actions.\\ : Provides a method for configuring custom actions on a listener, such as for Redirect Actions . The <ACTION NAME> in the annotation must match the serviceName in the ingress rules. The value of the annotation is the JSON spec of the action. See the Action type for documentation on what should be in the JSON. NOTE you must set the servicePort to use-annotation . For a fixed-response, use alb.ingress.kubernetes.io/actions.fixed-response-error: '{\"Type\": \"fixed-response\", \"FixedResponseConfig\": {\"ContentType\":\"text/plain\", \"StatusCode\":\"503\", \"MessageBody\":\"503 error text\"}}' with a serviceName : fixed - response - error and servicePort : use - annotation . For a HTTP to HTTPS redirect, use alb.ingress.kubernetes.io/actions.redirect: {\"Type\": \"redirect\", \"RedirectConfig\": { \"Protocol\": \"HTTPS\", \"StatusCode\": \"HTTP_301\"}} with serviceName : redirect and servicePort : use - annotation .","title":"Annotations"},{"location":"api/ingress/#services","text":"A subset of these annotations are supported on Services. This is used to customize the Target Group created for the Service. If a Service has no annotations, the Target Group options will default to the same options configured on the Ingress.","title":"Services"},{"location":"api/ingress/#optional-service-annotations","text":"alb.ingress.kubernetes.io/backend-protocol alb.ingress.kubernetes.io/healthcheck-interval-seconds alb.ingress.kubernetes.io/healthcheck-path alb.ingress.kubernetes.io/healthcheck-port alb.ingress.kubernetes.io/healthcheck-protocol alb.ingress.kubernetes.io/healthcheck-timeout-seconds alb.ingress.kubernetes.io/healthy-threshold-count alb.ingress.kubernetes.io/unhealthy-threshold-count alb.ingress.kubernetes.io/target-type alb.ingress.kubernetes.io/success-codes alb.ingress.kubernetes.io/target-group-attributes","title":"Optional Service Annotations"},{"location":"guide/controller/config/","text":"","title":"Configuration"},{"location":"guide/controller/how-it-works/","text":"How ALB ingress controller works \u00b6 Design \u00b6 The following diagram details the AWS components this controller creates. It also demonstrates the route ingress traffic takes from the ALB to the Kubernetes cluster. Ingress Creation \u00b6 This section describes each step (circle) above. This example demonstrates satisfying 1 ingress resource. [1] : The controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources. [2] : An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it's created in using annotations. [3] : Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource. [4] : Listeners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults ( 80 or 443 ) are used. Certificates may also be attached via annotations. [5] : Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service. Along with the above, the controller also... deletes AWS components when ingress resources are removed from k8s. modifies AWS components when ingress resources change in k8s. assembles a list of existing ingress-related AWS components on start-up, allowing you to recover if the controller were to be restarted. Ingress Traffic \u00b6 ALB Ingress controller supports two traffic modes: Instance mode IP mode By default, Instance mode is used, users can explicitly select the mode via alb.ingress.kubernetes.io/target-type annotation. Instance mode \u00b6 Ingress traffic starts at the ALB and reaches the Kubernetes nodes through each service's NodePort. This means that services referenced from ingress resources must be exposed by type : NodePort in order to be reached by the ALB. IP mode \u00b6 Ingress traffic starts at the ALB and reaches the Kubernetes pods directly. CNIs must support directly accessible POD ip via secondary IP addresses on ENI .","title":"How it works"},{"location":"guide/controller/how-it-works/#how-alb-ingress-controller-works","text":"","title":"How ALB ingress controller works"},{"location":"guide/controller/how-it-works/#design","text":"The following diagram details the AWS components this controller creates. It also demonstrates the route ingress traffic takes from the ALB to the Kubernetes cluster.","title":"Design"},{"location":"guide/controller/how-it-works/#ingress-creation","text":"This section describes each step (circle) above. This example demonstrates satisfying 1 ingress resource. [1] : The controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources. [2] : An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it's created in using annotations. [3] : Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource. [4] : Listeners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults ( 80 or 443 ) are used. Certificates may also be attached via annotations. [5] : Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service. Along with the above, the controller also... deletes AWS components when ingress resources are removed from k8s. modifies AWS components when ingress resources change in k8s. assembles a list of existing ingress-related AWS components on start-up, allowing you to recover if the controller were to be restarted.","title":"Ingress Creation"},{"location":"guide/controller/how-it-works/#ingress-traffic","text":"ALB Ingress controller supports two traffic modes: Instance mode IP mode By default, Instance mode is used, users can explicitly select the mode via alb.ingress.kubernetes.io/target-type annotation.","title":"Ingress Traffic"},{"location":"guide/controller/how-it-works/#instance-mode","text":"Ingress traffic starts at the ALB and reaches the Kubernetes nodes through each service's NodePort. This means that services referenced from ingress resources must be exposed by type : NodePort in order to be reached by the ALB.","title":"Instance mode"},{"location":"guide/controller/how-it-works/#ip-mode","text":"Ingress traffic starts at the ALB and reaches the Kubernetes pods directly. CNIs must support directly accessible POD ip via secondary IP addresses on ENI .","title":"IP mode"},{"location":"guide/controller/setup/","text":"Setup ALB ingress controller \u00b6 This document describes how to install ALB ingress controller into your kubernetes cluster on AWS. If you'd prefer an end-to-end walkthrough of setup instead, see the echoservice walkthrough Prerequisites \u00b6 This section details what must be setup in order for the controller to run. Kubelet \u00b6 The kubelet must be run with --cloud-provider=aws . This populates the EC2 instance ID in each node's spec. Role Permissions \u00b6 Adequate roles and policies must be configured in AWS and available to the node(s) running the controller. How access is granted is up to you. Some will attach the needed rights to node's role in AWS. Others will use projects like kube2iam . An example policy with the minimum rights can be found at iam-policy.json . Installation \u00b6 You can choose to install ALB ingress controller via Helm or Kubectl Helm \u00b6 Install Helm App Registry plugin Install ALB ingress controller helm registry install quay.io/coreos/alb-ingress-controller-helm Kubectl \u00b6 Download sample ALB ingress controller manifest wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/alb-ingress-controller.yaml Configure the ALB ingress controller manifest At minimum, edit the following variables: --cluster-name=devCluster : name of the cluster. AWS resources will be tagged with kubernetes.io/cluster/devCluster:owned Tip If ec2metadata is unavailable from the controller pod, edit the following variables: --aws-vpc-id=vpc-xxxxxx : vpc ID of the cluster. --aws-region=us-west-1 : AWS region of the cluster. Deploy the RBAC roles manifest kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/rbac-role.yaml Deploy the ALB ingress controller manifest kubectl apply -f alb-ingress-controller.yaml Verify the deployment was successful and the controller started kubectl logs -n kube-system $( kubectl get po -n kube-system | egrep -o alb-ingress [ a-zA-Z0-9- ] + ) Should display output similar to the following. ------------------------------------------------------------------------------- AWS ALB Ingress controller Release: 1.0.0 Build: git-7bc1850b Repository: https://github.com/kubernetes-sigs/aws-alb-ingress-controller.git -------------------------------------------------------------------------------","title":"Setup ALB ingress controller"},{"location":"guide/controller/setup/#setup-alb-ingress-controller","text":"This document describes how to install ALB ingress controller into your kubernetes cluster on AWS. If you'd prefer an end-to-end walkthrough of setup instead, see the echoservice walkthrough","title":"Setup ALB ingress controller"},{"location":"guide/controller/setup/#prerequisites","text":"This section details what must be setup in order for the controller to run.","title":"Prerequisites"},{"location":"guide/controller/setup/#kubelet","text":"The kubelet must be run with --cloud-provider=aws . This populates the EC2 instance ID in each node's spec.","title":"Kubelet"},{"location":"guide/controller/setup/#role-permissions","text":"Adequate roles and policies must be configured in AWS and available to the node(s) running the controller. How access is granted is up to you. Some will attach the needed rights to node's role in AWS. Others will use projects like kube2iam . An example policy with the minimum rights can be found at iam-policy.json .","title":"Role Permissions"},{"location":"guide/controller/setup/#installation","text":"You can choose to install ALB ingress controller via Helm or Kubectl","title":"Installation"},{"location":"guide/controller/setup/#helm","text":"Install Helm App Registry plugin Install ALB ingress controller helm registry install quay.io/coreos/alb-ingress-controller-helm","title":"Helm"},{"location":"guide/controller/setup/#kubectl","text":"Download sample ALB ingress controller manifest wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/alb-ingress-controller.yaml Configure the ALB ingress controller manifest At minimum, edit the following variables: --cluster-name=devCluster : name of the cluster. AWS resources will be tagged with kubernetes.io/cluster/devCluster:owned Tip If ec2metadata is unavailable from the controller pod, edit the following variables: --aws-vpc-id=vpc-xxxxxx : vpc ID of the cluster. --aws-region=us-west-1 : AWS region of the cluster. Deploy the RBAC roles manifest kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/rbac-role.yaml Deploy the ALB ingress controller manifest kubectl apply -f alb-ingress-controller.yaml Verify the deployment was successful and the controller started kubectl logs -n kube-system $( kubectl get po -n kube-system | egrep -o alb-ingress [ a-zA-Z0-9- ] + ) Should display output similar to the following. ------------------------------------------------------------------------------- AWS ALB Ingress controller Release: 1.0.0 Build: git-7bc1850b Repository: https://github.com/kubernetes-sigs/aws-alb-ingress-controller.git -------------------------------------------------------------------------------","title":"Kubectl"},{"location":"guide/external-dns/setup/","text":"Setup External DNS \u00b6 external-dns provisions DNS records based on the host information. This project will setup and manage records in Route 53 that point to controller deployed ALBs. Prerequisites \u00b6 Role Permissions \u00b6 Adequate roles and policies must be configured in AWS and available to the node(s) running the external-dns. See https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/aws.md#iam-permissions. Installation \u00b6 Download sample external-dns manifest wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/external-dns.yaml Edit the --domain-filter flag to include your hosted zone(s) The following example is for a hosted zone test-dns.com args : - --source=service - --source=ingress - --domain-filter=test-dns.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization Deploy external-dns kubectl apply -f external-dns.yaml Verify it deployed successfully. kubectl logs -f -n kube-system $( kubectl get po -n kube-system | egrep -o 'external-dns[A-Za-z0-9-]+' ) Should display output similar to the following. time = \"2017-09-19T02:51:54Z\" level = info msg = \"config: &{Master: KubeConfig: Sources:[service ingress] Namespace: FQDNTemplate: Compatibility: Provider:aws GoogleProject: DomainFilter:[] AzureConfigFile:/etc/kuberne tes/azure.json AzureResourceGroup: Policy:upsert-only Registry:txt TXTOwnerID:my-identifier TXTPrefix: Interval:1m0s Once:false DryRun:false LogFormat:text MetricsAddress::7979 Debug:false}\" time = \"2017-09-19T02:51:54Z\" level = info msg = \"Connected to cluster at https://10.3.0.1:443\"","title":"Setup External DNS"},{"location":"guide/external-dns/setup/#setup-external-dns","text":"external-dns provisions DNS records based on the host information. This project will setup and manage records in Route 53 that point to controller deployed ALBs.","title":"Setup External DNS"},{"location":"guide/external-dns/setup/#prerequisites","text":"","title":"Prerequisites"},{"location":"guide/external-dns/setup/#role-permissions","text":"Adequate roles and policies must be configured in AWS and available to the node(s) running the external-dns. See https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/aws.md#iam-permissions.","title":"Role Permissions"},{"location":"guide/external-dns/setup/#installation","text":"Download sample external-dns manifest wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/external-dns.yaml Edit the --domain-filter flag to include your hosted zone(s) The following example is for a hosted zone test-dns.com args : - --source=service - --source=ingress - --domain-filter=test-dns.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization Deploy external-dns kubectl apply -f external-dns.yaml Verify it deployed successfully. kubectl logs -f -n kube-system $( kubectl get po -n kube-system | egrep -o 'external-dns[A-Za-z0-9-]+' ) Should display output similar to the following. time = \"2017-09-19T02:51:54Z\" level = info msg = \"config: &{Master: KubeConfig: Sources:[service ingress] Namespace: FQDNTemplate: Compatibility: Provider:aws GoogleProject: DomainFilter:[] AzureConfigFile:/etc/kuberne tes/azure.json AzureResourceGroup: Policy:upsert-only Registry:txt TXTOwnerID:my-identifier TXTPrefix: Interval:1m0s Once:false DryRun:false LogFormat:text MetricsAddress::7979 Debug:false}\" time = \"2017-09-19T02:51:54Z\" level = info msg = \"Connected to cluster at https://10.3.0.1:443\"","title":"Installation"},{"location":"guide/ingress/annotation/","text":"Ingress annotations \u00b6 You can add kubernetes annotations to ingress and service objects to customize their behavior. Note Annotations applied to service have higher priority over annotations applied to ingress. Location column below indicates where that annotation can be applied to. Annotation keys and values can only be strings. Tip The annotation prefix can be changed using the --annotations-prefix command line argument, by default it's alb.ingress.kubernetes.io , as described in the table below. Name Type Default Location alb.ingress.kubernetes.io/healthcheck-interval-seconds integer 15 Ingress, Service alb.ingress.kubernetes.io/healthcheck-path string / Ingress, Service alb.ingress.kubernetes.io/healthcheck-port integer|traffic-port traffic-port Ingress, Service alb.ingress.kubernetes.io/healthcheck-protocol HTTP|HTTPS HTTP Ingress, Service alb.ingress.kubernetes.io/healthcheck-timeout-seconds integer 5 Ingress, Service alb.ingress.kubernetes.io/healthy-threshold-count integer 2 Ingress, Service alb.ingress.kubernetes.io/unhealthy-threshold-count integer 2 Ingress, Service Health Check \u00b6 Health check on target groups can be controlled with following annotations: alb.ingress.kubernetes.io/healthcheck-protocol controls the protocol used when performing health check on targets. Example alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS Tip default protocol can be set via --backend-protocol` flag alb.ingress.kubernetes.io/healthcheck-port controls the port used when performing health check on targets. Example alb.ingress.kubernetes.io/healthcheck-port: traffic-port alb.ingress.kubernetes.io/healthcheck-port: 80 alb.ingress.kubernetes.io/healthcheck-path controls the HTTP path when peforming health check on targets. Example alb.ingress.kubernetes.io/healthcheck-path: /ping alb.ingress.kubernetes.io/healthcheck-interval-seconds controls the interval(in seconds) between health check of an individual target. Example alb.ingress.kubernetes.io/healthcheck-interval-seconds: 10 alb.ingress.kubernetes.io/healthcheck-timeout-seconds controls the timeout(in seconds) during which no response from a target means a failed health check Example alb.ingress.kubernetes.io/healthcheck-timeout-seconds: 8 alb.ingress.kubernetes.io/healthy-threshold-count controls the consecutive health checks successes required before considering an unhealthy target healthy. Example alb.ingress.kubernetes.io/healthy-threshold-count: 2 alb.ingress.kubernetes.io/unhealthy-threshold-count controls the consecutive health check failures required before considering a target unhealthy. Example alb.ingress.kubernetes.io/unhealthy-threshold-count: 2","title":"Annotation"},{"location":"guide/ingress/annotation/#ingress-annotations","text":"You can add kubernetes annotations to ingress and service objects to customize their behavior. Note Annotations applied to service have higher priority over annotations applied to ingress. Location column below indicates where that annotation can be applied to. Annotation keys and values can only be strings. Tip The annotation prefix can be changed using the --annotations-prefix command line argument, by default it's alb.ingress.kubernetes.io , as described in the table below. Name Type Default Location alb.ingress.kubernetes.io/healthcheck-interval-seconds integer 15 Ingress, Service alb.ingress.kubernetes.io/healthcheck-path string / Ingress, Service alb.ingress.kubernetes.io/healthcheck-port integer|traffic-port traffic-port Ingress, Service alb.ingress.kubernetes.io/healthcheck-protocol HTTP|HTTPS HTTP Ingress, Service alb.ingress.kubernetes.io/healthcheck-timeout-seconds integer 5 Ingress, Service alb.ingress.kubernetes.io/healthy-threshold-count integer 2 Ingress, Service alb.ingress.kubernetes.io/unhealthy-threshold-count integer 2 Ingress, Service","title":"Ingress annotations"},{"location":"guide/ingress/annotation/#health-check","text":"Health check on target groups can be controlled with following annotations: alb.ingress.kubernetes.io/healthcheck-protocol controls the protocol used when performing health check on targets. Example alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS Tip default protocol can be set via --backend-protocol` flag alb.ingress.kubernetes.io/healthcheck-port controls the port used when performing health check on targets. Example alb.ingress.kubernetes.io/healthcheck-port: traffic-port alb.ingress.kubernetes.io/healthcheck-port: 80 alb.ingress.kubernetes.io/healthcheck-path controls the HTTP path when peforming health check on targets. Example alb.ingress.kubernetes.io/healthcheck-path: /ping alb.ingress.kubernetes.io/healthcheck-interval-seconds controls the interval(in seconds) between health check of an individual target. Example alb.ingress.kubernetes.io/healthcheck-interval-seconds: 10 alb.ingress.kubernetes.io/healthcheck-timeout-seconds controls the timeout(in seconds) during which no response from a target means a failed health check Example alb.ingress.kubernetes.io/healthcheck-timeout-seconds: 8 alb.ingress.kubernetes.io/healthy-threshold-count controls the consecutive health checks successes required before considering an unhealthy target healthy. Example alb.ingress.kubernetes.io/healthy-threshold-count: 2 alb.ingress.kubernetes.io/unhealthy-threshold-count controls the consecutive health check failures required before considering a target unhealthy. Example alb.ingress.kubernetes.io/unhealthy-threshold-count: 2","title":"Health Check"},{"location":"guide/ingress/spec/","text":"Ingress specification \u00b6 This document covers how ingress resources work in relation to The ALB Ingress Controller. An example ingress, from example is as follows. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : \"nginx-ingress\" namespace : \"2048-game\" annotations : kubernetes.io/ingress.class : alb labels : app : 2048-nginx-ingress spec : rules : - host : 2048.example.com http : paths : - path : / backend : serviceName : \"service-2048\" servicePort : 80 The host field specifies the eventual Route 53-managed domain that will route to this service. The service, service-2048, must be of type NodePort in order for the provisioned ALB to route to it.(see echoserver-service.yaml ) For details on purpose of annotations seen above, see Annotations .","title":"Spec"},{"location":"guide/ingress/spec/#ingress-specification","text":"This document covers how ingress resources work in relation to The ALB Ingress Controller. An example ingress, from example is as follows. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : \"nginx-ingress\" namespace : \"2048-game\" annotations : kubernetes.io/ingress.class : alb labels : app : 2048-nginx-ingress spec : rules : - host : 2048.example.com http : paths : - path : / backend : serviceName : \"service-2048\" servicePort : 80 The host field specifies the eventual Route 53-managed domain that will route to this service. The service, service-2048, must be of type NodePort in order for the provisioned ALB to route to it.(see echoserver-service.yaml ) For details on purpose of annotations seen above, see Annotations .","title":"Ingress specification"},{"location":"guide/walkthrough/echoserver/","text":"walkthrough: echoserver \u00b6 In this walkthrough, you'll Create a cluster with EKS Deploy an alb-ingress-controller Create deployments and ingress resources in the cluster Use external-dns to create a DNS record This assumes you have a route53 hosted zone available. Otherwise you can skip this, but you'll only be able to address the service from the ALB's DNS. Create the EKS cluster \u00b6 Install eksctl : https://eksctl.io Create EKS cluster via eksctl eksctl create cluster 2018-08-14T11:19:09-07:00 [\u2139] setting availability zones to [us-west-2c us-west-2a us-west-2b] 2018-08-14T11:19:09-07:00 [\u2139] importing SSH public key \"/Users/kamador/.ssh/id_rsa.pub\" as \"eksctl-exciting-gopher-1534270749-b7:71:da:f6:f3:63:7a:ee:ad:7a:10:37:28:ff:44:d1\" 2018-08-14T11:19:10-07:00 [\u2139] creating EKS cluster \"exciting-gopher-1534270749\" in \"us-west-2\" region 2018-08-14T11:19:10-07:00 [\u2139] creating ServiceRole stack \"EKS-exciting-gopher-1534270749-ServiceRole\" 2018-08-14T11:19:10-07:00 [\u2139] creating VPC stack \"EKS-exciting-gopher-1534270749-VPC\" 2018-08-14T11:19:50-07:00 [\u2714] created ServiceRole stack \"EKS-exciting-gopher-1534270749-ServiceRole\" 2018-08-14T11:20:30-07:00 [\u2714] created VPC stack \"EKS-exciting-gopher-1534270749-VPC\" 2018-08-14T11:20:30-07:00 [\u2139] creating control plane \"exciting-gopher-1534270749\" 2018-08-14T11:31:52-07:00 [\u2714] created control plane \"exciting-gopher-1534270749\" 2018-08-14T11:31:52-07:00 [\u2139] creating DefaultNodeGroup stack \"EKS-exciting-gopher-1534270749-DefaultNodeGroup\" 2018-08-14T11:35:33-07:00 [\u2714] created DefaultNodeGroup stack \"EKS-exciting-gopher-1534270749-DefaultNodeGroup\" 2018-08-14T11:35:33-07:00 [\u2714] all EKS cluster \"exciting-gopher-1534270749\" resources has been created 2018-08-14T11:35:33-07:00 [\u2714] saved kubeconfig as \"/Users/kamador/.kube/config\" 2018-08-14T11:35:34-07:00 [\u2139] the cluster has 0 nodes 2018-08-14T11:35:34-07:00 [\u2139] waiting for at least 2 nodes to become ready 2018-08-14T11:36:05-07:00 [\u2139] the cluster has 2 nodes 2018-08-14T11:36:05-07:00 [\u2139] node \"ip-192-168-139-176.us-west-2.compute.internal\" is ready 2018-08-14T11:36:05-07:00 [\u2139] node \"ip-192-168-214-126.us-west-2.compute.internal\" is ready 2018-08-14T11:36:05-07:00 [\u2714] EKS cluster \"exciting-gopher-1534270749\" in \"us-west-2\" region is ready Deploy the ALB ingress controller \u00b6 Download the example alb-ingress-manifest locally. wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/alb-ingress-controller.yaml wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/rbac-role.yaml Edit the manifest and set the following parameters and environment variables. cluster-name : name of the cluster. AWS_ACCESS_KEY_ID : access key id that alb controller can use to communicate with AWS. This is only used for convenience of this example. It will keep the credentials in plain text within this manifest. It's recommended a project such as kube2iam is used to resolve access. You will need to uncomment this from the manifest . - name : AWS_ACCESS_KEY_ID value : KEYVALUE AWS_SECRET_ACCESS_KEY : secret access key that alb controller can use to communicate with AWS. This is only used for convenience of this example. It will keep the credentials in plain text within this manifest. It's recommended a project such as kube2iam is used to resolve access. You will need to uncomment this from the manifest . - name : AWS_SECRET_ACCESS_KEY value : SECRETVALUE Deploy the modified alb-ingress-controller. kubectl apply -f rbac-role.yaml kubectl apply -f alb-ingress-controller.yaml The manifest above will deploy the controller to the kube-system namespace. Verify the deployment was successful and the controller started. kubectl logs -n kube-system $( kubectl get po -n kube-system | egrep -o alb-ingress [ a-zA-Z0-9- ] + ) Should display output similar to the following. ------------------------------------------------------------------------------- AWS ALB Ingress controller Release: UNKNOWN Build: UNKNOWN Repository: UNKNOWN ------------------------------------------------------------------------------- I0725 11:22:06.464996 16433 main.go:159] Creating API client for http://localhost:8001 I0725 11:22:06.563336 16433 main.go:203] Running in Kubernetes cluster version v1.8+ (v1.8.9+coreos.1) - git (clean) commit cd373fe93e046b0a0bc7e4045af1bf4171cea395 - platform linux/amd64 I0725 11:22:06.566255 16433 alb.go:80] ALB resource names will be prefixed with 2f92da62 I0725 11:22:06.645910 16433 alb.go:163] Starting AWS ALB Ingress controller Deploy the echoserver resources \u00b6 Deploy all the echoserver resources (namespace, service, deployment) kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-namespace.yaml && \\ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-service.yaml && \\ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-deployment.yaml && \\ List all the resources to ensure they were created. kubectl get -n echoserver deploy,svc Should resolve similar to the following. NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/echoserver 10.3.31.76 <nodes> 80:31027/TCP 4d NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/echoserver 1 1 1 1 4d Deploy ingress for echoserver \u00b6 Download the echoserver ingress manifest locally. wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-ingress.yaml Configure the subnets, either by add annotation to the ingress or add tags to subnets. Tip If you'd like to use external dns, alter the host field to a domain that you own in Route 53. Assuming you managed example.com in Route 53. Edit the alb.ingress.kubernetes.io/subnets annotation to include at least two subnets. eksctl get cluster exciting-gopher-1534270749 NAME VERSION STATUS CREATED VPC SUBNETS SECURITYGROUPS exciting-gopher-1534270749 1.10 ACTIVE 2018-08-14T18:20:32Z vpc-0aa01b07b3c922c9c subnet-05e1c98ed0f5b109e,subnet-07f5bb81f661df61b,subnet-0a4e6232630820516 sg-05ceb5eee9fd7cac4 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : echoserver namespace : echoserver annotations : alb.ingress.kubernetes.io/scheme : internet-facing alb.ingress.kubernetes.io/target-type : ip alb.ingress.kubernetes.io/subnets : subnet-05e1c98ed0f5b109e,subnet-07f5bb81f661df61b,subnet-0a4e6232630820516 alb.ingress.kubernetes.io/tags : Environment=dev,Team=test spec : rules : - host : echoserver.example.com http : paths : Adding tags to subnets for auto-discovery(instead of alb.ingress.kubernetes.io/subnets annotation) you must include the following tags on desired subnets. kubernetes.io/cluster/$CLUSTER_NAME where $CLUSTER_NAME is the same CLUSTER_NAME specified in the above step. kubernetes.io/role/internal-elb should be set to 1 or an empty tag value for internal load balancers. kubernetes.io/role/elb should be set to 1 or an empty tag value for internet-facing load balancers. An example of a subnet with the correct tags for the cluster joshcalico is as follows. Deploy the ingress resource for echoserver kubectl apply -f echoserver-ingress.yaml Verify the alb-ingress-controller creates the resources kubectl logs -n kube-system $( kubectl get po -n kube-system | egrep -o 'alb-ingress[a-zA-Z0-9-]+' ) | grep 'echoserver\\/echoserver' You should see similar to the following. echoserver/echoserver: Start ELBV2 (ALB) creation. echoserver/echoserver: Completed ELBV2 (ALB) creation. Name: joshcalico-echoserver-echo-2ad7 | ARN: arn:aws:elasticloadbalancing:us-east-2:432733164488:loadbalancer/app/joshcalico-echoserver-echo-2ad7/4579643c6f757be9 echoserver/echoserver: Start TargetGroup creation. echoserver/echoserver: Succeeded TargetGroup creation. ARN: arn:aws:elasticloadbalancing:us-east-2:432733164488:targetgroup/joshcalico-31027-HTTP-6657576/77ef58891a00263e | Name: joshcalico-31027-HTTP-6657576. echoserver/echoserver: Start Listener creation. echoserver/echoserver: Completed Listener creation. ARN: arn:aws:elasticloadbalancing:us-east-2:432733164488:listener/app/joshcalico-echoserver-echo-2ad7/4579643c6f757be9/2b2987fa3739c062 | Port: 80 | Proto: HTTP. echoserver/echoserver: Start Rule creation. echoserver/echoserver: Completed Rule creation. Rule Priority: \"1\" | Condition: [{ Field: \"host-header\", Values: [\"echoserver.joshrosso.com\"] },{ Field: \"path-pattern\", Values: [\"/\"] }] Check the events of the ingress to see what has occur. kubectl describe ing -n echoserver echoserver You should see similar to the following. Name: echoserver Namespace: echoserver Address: joshcalico-echoserver-echo-2ad7-1490890749.us-east-2.elb.amazonaws.com Default backend: default-http-backend:80 (10.2.1.28:8080) Rules: Host Path Backends ---- ---- -------- echoserver.joshrosso.com / echoserver:80 (<none>) Annotations: Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 3m 3m 1 ingress-controller Normal CREATE Ingress echoserver/echoserver 3m 32s 3 ingress-controller Normal UPDATE Ingress echoserver/echoserver The address seen above is the ALB's DNS record. This will be referenced via records created by external-dns. Setup external-DNS to manage DNS automatically \u00b6 Ensure your instance has the correct IAM permission required for external-dns. See https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/aws.md#iam-permissions. Download external-dns to manage Route 53. wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/external-dns.yaml Edit the --domain-filter flag to include your hosted zone(s) The following example is for a hosted zone test-dns.com args : - --source=service - --source=ingress - --domain-filter=test-dns.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization Verify the DNS has propagated dig echoserver.josh-test-dns.com ;; QUESTION SECTION: ;echoserver.josh-test-dns.com. IN A ;; ANSWER SECTION: echoserver.josh-test-dns.com. 60 IN A 13.59.147.105 echoserver.josh-test-dns.com. 60 IN A 18.221.65.39 echoserver.josh-test-dns.com. 60 IN A 52.15.186.25 Once it has, you can make a call to echoserver and it should return a response payload. curl echoserver.josh-test-dns.com CLIENT VALUES: client_address=10.0.50.185 command=GET real path=/ query=nil request_version=1.1 request_uri=http://echoserver.josh-test-dns.com:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=echoserver.josh-test-dns.com user-agent=curl/7.54.0 x-amzn-trace-id=Root=1-59c08da5-113347df69640735312371bd x-forwarded-for=67.173.237.250 x-forwarded-port=80 x-forwarded-proto=http BODY: Kube2iam setup \u00b6 follow below steps If you want to use kube2iam to provide the AWS credentials configure the proper policy The policy to be used can be fetched from https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/examples/iam-policy.json configure the proper role and create the trust relationship You have to find which role is associated woth your K8S nodes. Once you found take note of the full arn: arn : aws : iam :: XXXXXXXXXXXX : role / k8scluster - node create the role, called k8s-alb-controller, attach the above policy and add a Trust Relationship like: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:role/k8scluster-node\" }, \"Action\": \"sts:AssumeRole\" } ] } The new role will have the arn: arn : aws : iam ::: XXXXXXXXXXXX : role / k8s - alb - controller update the alb-ingress-controller.yaml Add the annotations in the template's metadata poin spec : replicas : 1 selector : matchLabels : app : alb-ingress-controller strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : annotations : iam.amazonaws.com/role : arn:aws:iam:::XXXXXXXXXXXX:role/k8s-alb-controller","title":"Echo server"},{"location":"guide/walkthrough/echoserver/#walkthrough-echoserver","text":"In this walkthrough, you'll Create a cluster with EKS Deploy an alb-ingress-controller Create deployments and ingress resources in the cluster Use external-dns to create a DNS record This assumes you have a route53 hosted zone available. Otherwise you can skip this, but you'll only be able to address the service from the ALB's DNS.","title":"walkthrough: echoserver"},{"location":"guide/walkthrough/echoserver/#create-the-eks-cluster","text":"Install eksctl : https://eksctl.io Create EKS cluster via eksctl eksctl create cluster 2018-08-14T11:19:09-07:00 [\u2139] setting availability zones to [us-west-2c us-west-2a us-west-2b] 2018-08-14T11:19:09-07:00 [\u2139] importing SSH public key \"/Users/kamador/.ssh/id_rsa.pub\" as \"eksctl-exciting-gopher-1534270749-b7:71:da:f6:f3:63:7a:ee:ad:7a:10:37:28:ff:44:d1\" 2018-08-14T11:19:10-07:00 [\u2139] creating EKS cluster \"exciting-gopher-1534270749\" in \"us-west-2\" region 2018-08-14T11:19:10-07:00 [\u2139] creating ServiceRole stack \"EKS-exciting-gopher-1534270749-ServiceRole\" 2018-08-14T11:19:10-07:00 [\u2139] creating VPC stack \"EKS-exciting-gopher-1534270749-VPC\" 2018-08-14T11:19:50-07:00 [\u2714] created ServiceRole stack \"EKS-exciting-gopher-1534270749-ServiceRole\" 2018-08-14T11:20:30-07:00 [\u2714] created VPC stack \"EKS-exciting-gopher-1534270749-VPC\" 2018-08-14T11:20:30-07:00 [\u2139] creating control plane \"exciting-gopher-1534270749\" 2018-08-14T11:31:52-07:00 [\u2714] created control plane \"exciting-gopher-1534270749\" 2018-08-14T11:31:52-07:00 [\u2139] creating DefaultNodeGroup stack \"EKS-exciting-gopher-1534270749-DefaultNodeGroup\" 2018-08-14T11:35:33-07:00 [\u2714] created DefaultNodeGroup stack \"EKS-exciting-gopher-1534270749-DefaultNodeGroup\" 2018-08-14T11:35:33-07:00 [\u2714] all EKS cluster \"exciting-gopher-1534270749\" resources has been created 2018-08-14T11:35:33-07:00 [\u2714] saved kubeconfig as \"/Users/kamador/.kube/config\" 2018-08-14T11:35:34-07:00 [\u2139] the cluster has 0 nodes 2018-08-14T11:35:34-07:00 [\u2139] waiting for at least 2 nodes to become ready 2018-08-14T11:36:05-07:00 [\u2139] the cluster has 2 nodes 2018-08-14T11:36:05-07:00 [\u2139] node \"ip-192-168-139-176.us-west-2.compute.internal\" is ready 2018-08-14T11:36:05-07:00 [\u2139] node \"ip-192-168-214-126.us-west-2.compute.internal\" is ready 2018-08-14T11:36:05-07:00 [\u2714] EKS cluster \"exciting-gopher-1534270749\" in \"us-west-2\" region is ready","title":"Create the EKS cluster"},{"location":"guide/walkthrough/echoserver/#deploy-the-alb-ingress-controller","text":"Download the example alb-ingress-manifest locally. wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/alb-ingress-controller.yaml wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/rbac-role.yaml Edit the manifest and set the following parameters and environment variables. cluster-name : name of the cluster. AWS_ACCESS_KEY_ID : access key id that alb controller can use to communicate with AWS. This is only used for convenience of this example. It will keep the credentials in plain text within this manifest. It's recommended a project such as kube2iam is used to resolve access. You will need to uncomment this from the manifest . - name : AWS_ACCESS_KEY_ID value : KEYVALUE AWS_SECRET_ACCESS_KEY : secret access key that alb controller can use to communicate with AWS. This is only used for convenience of this example. It will keep the credentials in plain text within this manifest. It's recommended a project such as kube2iam is used to resolve access. You will need to uncomment this from the manifest . - name : AWS_SECRET_ACCESS_KEY value : SECRETVALUE Deploy the modified alb-ingress-controller. kubectl apply -f rbac-role.yaml kubectl apply -f alb-ingress-controller.yaml The manifest above will deploy the controller to the kube-system namespace. Verify the deployment was successful and the controller started. kubectl logs -n kube-system $( kubectl get po -n kube-system | egrep -o alb-ingress [ a-zA-Z0-9- ] + ) Should display output similar to the following. ------------------------------------------------------------------------------- AWS ALB Ingress controller Release: UNKNOWN Build: UNKNOWN Repository: UNKNOWN ------------------------------------------------------------------------------- I0725 11:22:06.464996 16433 main.go:159] Creating API client for http://localhost:8001 I0725 11:22:06.563336 16433 main.go:203] Running in Kubernetes cluster version v1.8+ (v1.8.9+coreos.1) - git (clean) commit cd373fe93e046b0a0bc7e4045af1bf4171cea395 - platform linux/amd64 I0725 11:22:06.566255 16433 alb.go:80] ALB resource names will be prefixed with 2f92da62 I0725 11:22:06.645910 16433 alb.go:163] Starting AWS ALB Ingress controller","title":"Deploy the ALB ingress controller"},{"location":"guide/walkthrough/echoserver/#deploy-the-echoserver-resources","text":"Deploy all the echoserver resources (namespace, service, deployment) kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-namespace.yaml && \\ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-service.yaml && \\ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-deployment.yaml && \\ List all the resources to ensure they were created. kubectl get -n echoserver deploy,svc Should resolve similar to the following. NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/echoserver 10.3.31.76 <nodes> 80:31027/TCP 4d NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/echoserver 1 1 1 1 4d","title":"Deploy the echoserver resources"},{"location":"guide/walkthrough/echoserver/#deploy-ingress-for-echoserver","text":"Download the echoserver ingress manifest locally. wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/echoservice/echoserver-ingress.yaml Configure the subnets, either by add annotation to the ingress or add tags to subnets. Tip If you'd like to use external dns, alter the host field to a domain that you own in Route 53. Assuming you managed example.com in Route 53. Edit the alb.ingress.kubernetes.io/subnets annotation to include at least two subnets. eksctl get cluster exciting-gopher-1534270749 NAME VERSION STATUS CREATED VPC SUBNETS SECURITYGROUPS exciting-gopher-1534270749 1.10 ACTIVE 2018-08-14T18:20:32Z vpc-0aa01b07b3c922c9c subnet-05e1c98ed0f5b109e,subnet-07f5bb81f661df61b,subnet-0a4e6232630820516 sg-05ceb5eee9fd7cac4 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : echoserver namespace : echoserver annotations : alb.ingress.kubernetes.io/scheme : internet-facing alb.ingress.kubernetes.io/target-type : ip alb.ingress.kubernetes.io/subnets : subnet-05e1c98ed0f5b109e,subnet-07f5bb81f661df61b,subnet-0a4e6232630820516 alb.ingress.kubernetes.io/tags : Environment=dev,Team=test spec : rules : - host : echoserver.example.com http : paths : Adding tags to subnets for auto-discovery(instead of alb.ingress.kubernetes.io/subnets annotation) you must include the following tags on desired subnets. kubernetes.io/cluster/$CLUSTER_NAME where $CLUSTER_NAME is the same CLUSTER_NAME specified in the above step. kubernetes.io/role/internal-elb should be set to 1 or an empty tag value for internal load balancers. kubernetes.io/role/elb should be set to 1 or an empty tag value for internet-facing load balancers. An example of a subnet with the correct tags for the cluster joshcalico is as follows. Deploy the ingress resource for echoserver kubectl apply -f echoserver-ingress.yaml Verify the alb-ingress-controller creates the resources kubectl logs -n kube-system $( kubectl get po -n kube-system | egrep -o 'alb-ingress[a-zA-Z0-9-]+' ) | grep 'echoserver\\/echoserver' You should see similar to the following. echoserver/echoserver: Start ELBV2 (ALB) creation. echoserver/echoserver: Completed ELBV2 (ALB) creation. Name: joshcalico-echoserver-echo-2ad7 | ARN: arn:aws:elasticloadbalancing:us-east-2:432733164488:loadbalancer/app/joshcalico-echoserver-echo-2ad7/4579643c6f757be9 echoserver/echoserver: Start TargetGroup creation. echoserver/echoserver: Succeeded TargetGroup creation. ARN: arn:aws:elasticloadbalancing:us-east-2:432733164488:targetgroup/joshcalico-31027-HTTP-6657576/77ef58891a00263e | Name: joshcalico-31027-HTTP-6657576. echoserver/echoserver: Start Listener creation. echoserver/echoserver: Completed Listener creation. ARN: arn:aws:elasticloadbalancing:us-east-2:432733164488:listener/app/joshcalico-echoserver-echo-2ad7/4579643c6f757be9/2b2987fa3739c062 | Port: 80 | Proto: HTTP. echoserver/echoserver: Start Rule creation. echoserver/echoserver: Completed Rule creation. Rule Priority: \"1\" | Condition: [{ Field: \"host-header\", Values: [\"echoserver.joshrosso.com\"] },{ Field: \"path-pattern\", Values: [\"/\"] }] Check the events of the ingress to see what has occur. kubectl describe ing -n echoserver echoserver You should see similar to the following. Name: echoserver Namespace: echoserver Address: joshcalico-echoserver-echo-2ad7-1490890749.us-east-2.elb.amazonaws.com Default backend: default-http-backend:80 (10.2.1.28:8080) Rules: Host Path Backends ---- ---- -------- echoserver.joshrosso.com / echoserver:80 (<none>) Annotations: Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 3m 3m 1 ingress-controller Normal CREATE Ingress echoserver/echoserver 3m 32s 3 ingress-controller Normal UPDATE Ingress echoserver/echoserver The address seen above is the ALB's DNS record. This will be referenced via records created by external-dns.","title":"Deploy ingress for echoserver"},{"location":"guide/walkthrough/echoserver/#setup-external-dns-to-manage-dns-automatically","text":"Ensure your instance has the correct IAM permission required for external-dns. See https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/aws.md#iam-permissions. Download external-dns to manage Route 53. wget https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/master/docs/examples/external-dns.yaml Edit the --domain-filter flag to include your hosted zone(s) The following example is for a hosted zone test-dns.com args : - --source=service - --source=ingress - --domain-filter=test-dns.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization Verify the DNS has propagated dig echoserver.josh-test-dns.com ;; QUESTION SECTION: ;echoserver.josh-test-dns.com. IN A ;; ANSWER SECTION: echoserver.josh-test-dns.com. 60 IN A 13.59.147.105 echoserver.josh-test-dns.com. 60 IN A 18.221.65.39 echoserver.josh-test-dns.com. 60 IN A 52.15.186.25 Once it has, you can make a call to echoserver and it should return a response payload. curl echoserver.josh-test-dns.com CLIENT VALUES: client_address=10.0.50.185 command=GET real path=/ query=nil request_version=1.1 request_uri=http://echoserver.josh-test-dns.com:8080/ SERVER VALUES: server_version=nginx: 1.10.0 - lua: 10001 HEADERS RECEIVED: accept=*/* host=echoserver.josh-test-dns.com user-agent=curl/7.54.0 x-amzn-trace-id=Root=1-59c08da5-113347df69640735312371bd x-forwarded-for=67.173.237.250 x-forwarded-port=80 x-forwarded-proto=http BODY:","title":"Setup external-DNS to manage DNS automatically"},{"location":"guide/walkthrough/echoserver/#kube2iam-setup","text":"follow below steps If you want to use kube2iam to provide the AWS credentials configure the proper policy The policy to be used can be fetched from https://github.com/kubernetes-sigs/aws-alb-ingress-controller/blob/master/docs/examples/iam-policy.json configure the proper role and create the trust relationship You have to find which role is associated woth your K8S nodes. Once you found take note of the full arn: arn : aws : iam :: XXXXXXXXXXXX : role / k8scluster - node create the role, called k8s-alb-controller, attach the above policy and add a Trust Relationship like: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:role/k8scluster-node\" }, \"Action\": \"sts:AssumeRole\" } ] } The new role will have the arn: arn : aws : iam ::: XXXXXXXXXXXX : role / k8s - alb - controller update the alb-ingress-controller.yaml Add the annotations in the template's metadata poin spec : replicas : 1 selector : matchLabels : app : alb-ingress-controller strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : annotations : iam.amazonaws.com/role : arn:aws:iam:::XXXXXXXXXXXX:role/k8s-alb-controller","title":"Kube2iam setup"}]}